{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset class for all related training subclasses and variables\n",
    "    \"\"\"\n",
    "    classes = [\"ham\", \"spam\"]  # ham = 0 for not spam, spam = 1 for is spam\n",
    "    train_set = None\n",
    "    validation_set = None\n",
    "    test_set = None\n",
    "    weights = None\n",
    "    learning_rate = .01  # Natural learning rate constant\n",
    "    penalty = 0.001  # penalty (lambda) constant\n",
    "    epochs = 10  # Number of iterations\n",
    "\n",
    "    def learn_weights(self):\n",
    "        \"\"\"\n",
    "        Learn weights by using gradient ascent\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Adjust each weight...\n",
    "        for weight_string in self.weights:\n",
    "            weight_sum = 0.0\n",
    "\n",
    "            # ...using all training instances\n",
    "            for key in self.train_set:\n",
    "                doc = self.train_set[key]\n",
    "                # y_sample is true y value (classification) of the doc\n",
    "                y_sample = 0.0\n",
    "\n",
    "                if doc.true_class == self.classes[1]:\n",
    "                    y_sample = 1.0\n",
    "\n",
    "                # Only add to the sum if the doc contains the token (the count of it would be 0 anyways)\n",
    "                if weight_string in doc.word_bag:\n",
    "                    probability = self.predict_probability(self.classes[1], doc)\n",
    "                    weight_sum += float(doc.word_bag[weight_string]) * (y_sample - probability)\n",
    "\n",
    "            new_weight = self.weights[weight_string] + (\n",
    "                    (self.learning_rate * weight_sum) - (float(self.penalty) * self.weights[weight_string]))\n",
    "            # print(\"Weight for {} is {}, adjusted to {}\".format(weight_string, self.weights[weight_string], new_weight))\n",
    "            self.weights[weight_string] = new_weight\n",
    "\n",
    "    def train_loop(self):\n",
    "        \"\"\"\n",
    "        # Adjust weights for every epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        dataset.learn_weights()\n",
    "        for x in range(0, self.epochs):\n",
    "            print('Epoch {}/{}'.format(x, self.epochs))\n",
    "            dataset.learn_weights()\n",
    "\n",
    "    def predict_probability(self, guess, doc):\n",
    "        \"\"\"\n",
    "        Calculate conditional probability for the specified doc. Where class_prob is 1|X or 0|X\n",
    "        1 is spam and 0 is ham\n",
    "        :param guess:\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Total tokens in doc. Used to normalize word counts to stay within 0 and 1 for avoiding overflow\n",
    "        weight_sum = self.weights['weight_zero']\n",
    "        for word in doc.word_bag:\n",
    "            # print(word)\n",
    "            if word in self.weights:\n",
    "                # print(word)\n",
    "                # sum of weights * token count for each token in document\n",
    "                count = float(doc.word_bag[word])\n",
    "                weight_sum += self.weights[word] * count\n",
    "            # else:\n",
    "            #     # self.weights[word] = 0.0\n",
    "            #     print(\"{} not weighted\".format(word))\n",
    "\n",
    "        result = None\n",
    "        exponent = math.exp(float(weight_sum))\n",
    "        if guess == self.classes[0]:\n",
    "            result = 1.0 / (1.0 + exponent)\n",
    "        elif guess == self.classes[1]:\n",
    "            result = exponent / (1.0 + exponent)\n",
    "        return result\n",
    "\n",
    "    def apply_logistic_regression(self, doc):\n",
    "        \"\"\"\n",
    "        Apply algorithm to guess class for specific instance of test set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        score = dict()\n",
    "        score[0] = self.predict_probability(self.classes[0], doc)\n",
    "        score[1] = self.predict_probability(self.classes[1], doc)\n",
    "        if score[1] > score[0]:\n",
    "            return self.classes[1]\n",
    "        else:\n",
    "            return self.classes[0]\n",
    "\n",
    "    def logistic_regression(self):\n",
    "        # Apply algorithm on test set\n",
    "        correct_guesses = 0.0\n",
    "        for key in self.test_set:\n",
    "            doc = self.test_set[key]\n",
    "            doc.learned_class = (self.apply_logistic_regression(doc))\n",
    "            if doc.learned_class == doc.true_class:\n",
    "                correct_guesses += 1.0\n",
    "\n",
    "        filtered_test_set = filter_doc_set(self.test_set)\n",
    "\n",
    "        # Apply algorithm on filtered test set\n",
    "        correct_guesses_filtered = 0.0\n",
    "        for key in filtered_test_set:\n",
    "            doc = filtered_test_set[key]\n",
    "            doc.learned_class = self.apply_logistic_regression(doc)\n",
    "            if doc.learned_class == doc.true_class:\n",
    "                correct_guesses_filtered += 1.0\n",
    "\n",
    "        print(\"Correct guesses before filtering stop words:\\t%d/%s\" % (correct_guesses, len(self.test_set)))\n",
    "        print(\"Accuracy before filtering stop words:\\t\\t\\t%.4f%%\" % (\n",
    "                100.0 * float(correct_guesses) / float(len(self.test_set))))\n",
    "        print(\n",
    "            \"Correct guesses after filtering stop words:\\t\\t%d/%s\" % (correct_guesses_filtered, len(filtered_test_set)))\n",
    "        print(\"Accuracy after filtering stop words:\\t\\t\\t%.4f%%\" % (\n",
    "                100.0 * float(correct_guesses_filtered) / float(len(filtered_test_set))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"\n",
    "    Document class to store email instances easier\n",
    "    \"\"\"\n",
    "    title = None\n",
    "    # text = None\n",
    "    word_bag = None\n",
    "    true_class = None  # spam or ham\n",
    "\n",
    "    # weights = {'weight_zero': 1.0}  # x0 assumed 1 for all documents (training examples)\n",
    "    weights = None\n",
    "    learned_class = None  # spam or ham\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,\n",
    "                 title,\n",
    "                 # text,\n",
    "                 bag,\n",
    "                 true_class):\n",
    "        # self.text = text\n",
    "        self.word_bag = bag\n",
    "        self.true_class = true_class\n",
    "        self.title = title"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    counts frequency of each word in given text\n",
    "    order of sequence doesn't matter\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bags_of_words = collections.Counter(re.findall(r'\\w+', text))\n",
    "    return dict(bags_of_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_set(directory, true_class, split):\n",
    "    \"\"\"\n",
    "    Read all text files in the given directory and construct the data set, D\n",
    "    :param directory: the directory path should just be like \"train/ham\" for example\n",
    "    :param true_class: True class is the true classification of the email (spam or ham)\n",
    "    :param split: whether or not to split the dataset into a second validation set\n",
    "    :return: the dictionary to store the email in\n",
    "    \"\"\"\n",
    "    # if split:\n",
    "    #     print(\"splitting dataset\")\n",
    "    #         # dataset.validation_set =\n",
    "    #     print(int(len(dataset.filtered_training_set) * .7))\n",
    "\n",
    "    dictionary = dict()\n",
    "    dictionary_2 = dict()\n",
    "    file_list = os.listdir(directory)\n",
    "    total = len(file_list)\n",
    "    part_1 = int(total * .7)\n",
    "    part_2 = total - part_1\n",
    "    # print(part_1)\n",
    "    # print(part_2)\n",
    "    count = 0\n",
    "\n",
    "    for dir_entry in file_list:\n",
    "        dir_entry_path = os.path.join(directory, dir_entry)\n",
    "        if os.path.isfile(dir_entry_path):\n",
    "            with open(dir_entry_path, 'r', errors='ignore') as text_file:\n",
    "                # stores dictionary of dictionary of dictionary as explained above in the initialization\n",
    "                key = dir_entry[:4]\n",
    "                # print(key)\n",
    "                title = dir_entry\n",
    "                text = text_file.read()\n",
    "                bag = bag_of_words(text)\n",
    "                # print(count)\n",
    "                if split & (count >= part_1):\n",
    "                    # print(\"adding to dict2\")\n",
    "                    dictionary_2.update({key: Document(title, bag, true_class)})\n",
    "                else:\n",
    "                    # print(\"adding to dict1\")\n",
    "                    dictionary.update({key: Document(title, bag, true_class)})\n",
    "                count += 1\n",
    "    # print(\"dataset built!\")\n",
    "    return dictionary, dictionary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def set_stop_words():\n",
    "    \"\"\"\n",
    "    Set the stop words\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    stop_words_dir = 'stop_words.txt'\n",
    "    with open(stop_words_dir, 'r') as txt:\n",
    "        stops = (txt.read().splitlines())\n",
    "    return stops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def remove_stop_words(stops, data_set):\n",
    "    \"\"\"\n",
    "    Remove stop words from data set and store in dictionary\n",
    "    :param stops:\n",
    "    :param data_set:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    filtered_data_set = copy.deepcopy(data_set)\n",
    "    for i in stops:\n",
    "        for j in filtered_data_set:\n",
    "            if i in filtered_data_set[j].word_bag:\n",
    "                del filtered_data_set[j].word_bag[i]\n",
    "    return filtered_data_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def extract_vocab(data_set):\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary of all the text in a data set from a word_bag\n",
    "    :param data_set:\n",
    "    :return: v, a list of vocabulary\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    for i in data_set:\n",
    "        for j in data_set[i].word_bag:\n",
    "            if j not in v:\n",
    "                v.append(j)\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def filter_doc_set(doc_set):\n",
    "    # Set the stop words list\n",
    "    stop_words = set_stop_words()\n",
    "\n",
    "    # Set up filtered sets without stop words\n",
    "    filtered_doc_set = remove_stop_words(stop_words, doc_set)\n",
    "    return filtered_doc_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def build_data_sets(train_spam_dir, train_ham_dir, test_spam_dir, test_ham_dir):\n",
    "    \"\"\"\n",
    "    takes directories holding the data text files as parameters. \"train/ham\" for example\n",
    "    :param classes:\n",
    "    :param train_spam_dir:\n",
    "    :param train_ham_dir:\n",
    "    :param test_spam_dir:\n",
    "    :param test_ham_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = Dataset()\n",
    "\n",
    "    # Stores emails as dictionaries. email_file_name : Document (class defined below)\n",
    "    dataset.train_set = dict()\n",
    "    dataset.test_set = dict()\n",
    "\n",
    "    dataset.validation_set = dict()\n",
    "\n",
    "    # Set up initial data sets. Dictionaries containing the text, word frequencies, and true/learned classifications\n",
    "    train_set_spam, validate_set_spam = make_data_set(train_spam_dir, dataset.classes[1], True)\n",
    "    train_set_ham, validate_set_ham = make_data_set(train_ham_dir, dataset.classes[0], True)\n",
    "    test_set_spam, temp1 = make_data_set(test_spam_dir, dataset.classes[1], False)\n",
    "    test_set_ham, temp2 = make_data_set(test_ham_dir, dataset.classes[0], False)\n",
    "\n",
    "    dataset.train_set.update(train_set_spam)\n",
    "    dataset.train_set.update(train_set_ham)\n",
    "\n",
    "    dataset.validation_set.update(validate_set_spam)\n",
    "    dataset.validation_set.update(validate_set_ham)\n",
    "\n",
    "    dataset.test_set.update(test_set_spam)\n",
    "    dataset.test_set.update(test_set_ham)\n",
    "\n",
    "    filtered_training_set = filter_doc_set(dataset.train_set)\n",
    "\n",
    "    # Extract training set vocabulary/tokens in the training set\n",
    "    # training_set_vocab = extract_vocab(dataset.train_set)\n",
    "    filtered_training_set_vocab = extract_vocab(filtered_training_set)\n",
    "\n",
    "    # build initial weights from filtered vocab\n",
    "    # store weights as dictionary. w0 initially 0.0, others initially 0.0. token : weight value\n",
    "    dataset.weights = {'weight_zero': 0.0}\n",
    "    # filtered_weights = {'weight_zero': 0.0}\n",
    "\n",
    "    # Set all weights in training set vocabulary to be initially 0.0. w0 ('weight_zero') is initially 0.0\n",
    "    # for i in training_set_vocab:\n",
    "    #     weights[i] = 0.0\n",
    "    for i in filtered_training_set_vocab:\n",
    "        dataset.weights[i] = 0.0\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Dataset 1\n",
      "Start Dataset 2\n",
      "Start Dataset 3\n"
     ]
    }
   ],
   "source": [
    "dataset_roots = [\"data/dataset 1/\", \"data/dataset 2/\", \"data/dataset 3/\"]\n",
    "\n",
    "for i in range(len(dataset_roots)):\n",
    "    training_spam_dir = dataset_roots[i] + \"train/spam\"\n",
    "    training_ham_dir = dataset_roots[i] + \"train/ham\"\n",
    "    test_spam_dir = dataset_roots[i] + \"test/spam\"\n",
    "    test_ham_dir = dataset_roots[i] + \"test/ham\"\n",
    "    print(\"Start Dataset {}\".format(i + 1))\n",
    "    dataset = build_data_sets(training_spam_dir, training_ham_dir, test_spam_dir, test_ham_dir)\n",
    "    # dataset.split_training_set()\n",
    "    # dataset.train_loop()\n",
    "    # dataset.logistic_regression()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"\n",
    "    Test/debugging\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset_root = \"data/dataset 1/\"\n",
    "    training_spam_dir = dataset_root + \"train/spam\"\n",
    "    training_ham_dir = dataset_root + \"train/ham\"\n",
    "    test_spam_dir = dataset_root + \"test/spam\"\n",
    "    test_ham_dir = dataset_root + \"test/ham\"\n",
    "\n",
    "    temp_bag = bag_of_words(\"hello world it is i Achilles\")\n",
    "\n",
    "    temp_path = \"data/dataset 1/train/spam\"\n",
    "    temp_class = \"spam\"\n",
    "    temp_dataset = make_data_set(temp_path, temp_class)\n",
    "\n",
    "    set_stop_words()\n",
    "\n",
    "    temp_path = \"data/dataset 1/train/spam\"\n",
    "    temp_class = \"spam\"\n",
    "    remove_stop_words(set_stop_words(), temp_dataset)\n",
    "\n",
    "    extract_vocab(temp_dataset)\n",
    "\n",
    "    temp_training_spam_dir = \"data/dataset 1/train/spam\"\n",
    "    temp_training_ham_dir = \"data/dataset 1/train/ham\"\n",
    "    temp_test_spam_dir = \"data/dataset 1/test/spam\"\n",
    "    temp_test_ham_dir = \"data/dataset 1/test/ham\"\n",
    "    temp_datasets = build_data_sets(temp_training_spam_dir, temp_training_ham_dir, temp_test_spam_dir,\n",
    "                                    temp_test_ham_dir)\n",
    "\n",
    "    temp_doc = temp_datasets.train_set[\"0100\"]\n",
    "    temp_guess = \"spam\"\n",
    "    temp_datasets.predict_probability(temp_guess, temp_doc)\n",
    "\n",
    "    temp_datasets.learn_weights()\n",
    "\n",
    "    temp_datasets.logistic_regression()\n",
    "\n",
    "    # learn_weights(temp_datasets, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}