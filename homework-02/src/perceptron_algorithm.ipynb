{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to execute\n",
    "\n",
    "#python perceptron_algorithm.py <training_directory> <testing_directory> <number_iterations> <learning_constant>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document class to store email instances\n",
    "class Document:\n",
    "    text = \"\"\n",
    "    word_freqs = {}\n",
    "\n",
    "    # spam or ham\n",
    "    true_class = \"\"\n",
    "    learned_class = \"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, text, counter, true_class):\n",
    "        self.text = text\n",
    "        self.word_freqs = counter\n",
    "        self.true_class = true_class\n",
    "\n",
    "    def getText(self):\n",
    "        return self.text\n",
    "\n",
    "    def getWordFreqs(self):\n",
    "        return self.word_freqs\n",
    "\n",
    "    def getTrueClass(self):\n",
    "        return self.true_class\n",
    "\n",
    "    def getLearnedClass(self):\n",
    "        return self.learned_class\n",
    "\n",
    "    def setLearnedClass(self, guess):\n",
    "        self.learned_class = guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts frequency of each word in the text files and order of sequence doesn't matter\n",
    "def bagOfWords(text):\n",
    "    bagsofwords = collections.Counter(re.findall(r'\\w+', text))\n",
    "    return dict(bagsofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all text files in given directory and construct the data set\n",
    "# the directory path should just be like \"train/ham\" for example\n",
    "# storage is the dictionary to store the email in\n",
    "# True class is the true classification of the email (spam or ham)\n",
    "\n",
    "def makeDataSet(storage_dict, directory, true_class):\n",
    "    for dir_entry in os.listdir(directory):\n",
    "        dir_entry_path = os.path.join(directory, dir_entry)\n",
    "        if os.path.isfile(dir_entry_path):\n",
    "            with open(dir_entry_path, 'r') as text_file:\n",
    "                # stores dictionary of dictionary of dictionary as explained above in the initialization\n",
    "                text = text_file.read()\n",
    "                storage_dict.update({dir_entry_path: Document(text, bagOfWords(text), true_class)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stop words\n",
    "def setStopWords(stop_word_text_file):\n",
    "    stops = []\n",
    "    with open(stop_word_text_file, 'r') as txt:\n",
    "        stops = (txt.read().splitlines())\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from data set and store in dictionary\n",
    "def removeStopWords(stops, data_set):\n",
    "    filtered_data_set = copy.deepcopy(data_set)\n",
    "    for i in stops:\n",
    "        for j in filtered_data_set:\n",
    "            if i in filtered_data_set[j].getWordFreqs():\n",
    "                del filtered_data_set[j].getWordFreqs()[i]\n",
    "    return filtered_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary of all the text in a data set\n",
    "def extractVocab(data_set):\n",
    "    v = []\n",
    "    for i in data_set:\n",
    "        for j in data_set[i].getWordFreqs():\n",
    "            if j not in v:\n",
    "                v.append(j)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learns weights using the perceptron training rule\n",
    "def learnWeights(weights, learning_constant, training_set, num_iterations, classes):\n",
    "    # Adjust weights num_iterations times\n",
    "    for i in num_iterations:\n",
    "        # Go through all training instances and update weights\n",
    "        for d in training_set:\n",
    "            # Used to get the current perceptron's output. If > 0, then spam, else output ham.\n",
    "            weight_sum = weights['weight_zero']\n",
    "            for f in training_set[d].getWordFreqs():\n",
    "                if f not in weights:\n",
    "                    weights[f] = 0.0\n",
    "                weight_sum += weights[f] * training_set[d].getWordFreqs()[f]\n",
    "            perceptron_output = 0.0\n",
    "            if weight_sum > 0:\n",
    "                perceptron_output = 1.0\n",
    "            target_value = 0.0\n",
    "            if training_set[d].getTrueClass() == classes[1]:\n",
    "                target_value = 1.0\n",
    "            # Update all weights that are relevant to the instance at hand\n",
    "            for w in training_set[d].getWordFreqs():\n",
    "                weights[w] += float(learning_constant) * float((target_value - perceptron_output)) * \\\n",
    "                                float(training_set[d].getWordFreqs()[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the algorithm to test accuracy on the test set. Returns the perceptron output\n",
    "def apply(weights, classes, instance):\n",
    "    weight_sum = weights['weight_zero']\n",
    "    for i in instance.getWordFreqs():\n",
    "        if i not in weights:\n",
    "            weights[i] = 0.0\n",
    "        weight_sum += weights[i] * instance.getWordFreqs()[i]\n",
    "    if weight_sum > 0:\n",
    "        # return is spam\n",
    "        return 1\n",
    "    else:\n",
    "        # return is ham\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Takes training directory containing spam and ham folder. Same with test directory\n",
    "# Also takes number of iterations and learning rate as parameters\n",
    "def main(train_dir, test_dir, iterations, learning_constant):\n",
    "    # Create dictionaries and lists needed\n",
    "    training_set = {}\n",
    "    test_set = {}\n",
    "    filtered_training_set = {}\n",
    "    filtered_test_set = {}\n",
    "\n",
    "    # Stop words to filter out\n",
    "    stop_words = setStopWords('stop_words.txt')\n",
    "\n",
    "    # ham = 0 for not spam, spam = 1 for is spam\n",
    "    classes = [\"ham\", \"spam\"]\n",
    "\n",
    "    # Number of iterations and learning constant (usually around .1 or .01)\n",
    "    iterations = iterations\n",
    "    learning_constant = learning_constant\n",
    "\n",
    "    # Set up data sets. Dictionaries containing the text, word frequencies, and true/learned classifications\n",
    "    makeDataSet(training_set, train_dir + \"/spam\", classes[1])\n",
    "    makeDataSet(training_set, train_dir + \"/ham\", classes[0])\n",
    "    makeDataSet(test_set, test_dir + \"/spam\", classes[1])\n",
    "    makeDataSet(test_set, test_dir + \"/ham\", classes[0])\n",
    "\n",
    "    # Set up data sets without stop words\n",
    "    filtered_training_set = removeStopWords(stop_words, training_set)\n",
    "    filtered_test_set = removeStopWords(stop_words, test_set)\n",
    "\n",
    "    # Extract training set vocabulary\n",
    "    training_set_vocab = extractVocab(training_set)\n",
    "    filtered_training_set_vocab = extractVocab(filtered_training_set)\n",
    "\n",
    "    # store weights as dictionary. w0 initiall 1.0, others initially 1.0. token : weight value\n",
    "    weights = {'weight_zero': 1.0}\n",
    "    filtered_weights = {'weight_zero': 1.0}\n",
    "    for i in training_set_vocab:\n",
    "        weights[i] = 0.0\n",
    "    for i in filtered_training_set_vocab:\n",
    "        filtered_weights[i] = 0.0\n",
    "\n",
    "    # Learn weights using the training_set and the filtered_training_set\n",
    "    learnWeights(weights, learning_constant, training_set, iterations, classes)\n",
    "    learnWeights(filtered_weights, learning_constant, filtered_training_set, iterations, classes)\n",
    "\n",
    "    #Apply the algorithm on the test set and report accuracy\n",
    "    num_correct_guesses = 0\n",
    "    for i in test_set:\n",
    "        guess = apply(weights, classes, test_set[i])\n",
    "        if guess == 1:\n",
    "            test_set[i].setLearnedClass(classes[1])\n",
    "            if test_set[i].getTrueClass() == test_set[i].getLearnedClass():\n",
    "                num_correct_guesses += 1\n",
    "        if guess == 0:\n",
    "            test_set[i].setLearnedClass(classes[0])\n",
    "            if test_set[i].getTrueClass() == test_set[i].getLearnedClass():\n",
    "                num_correct_guesses += 1\n",
    "\n",
    "    # Apply algorithm again on test set without any stop words and report accuracy\n",
    "    filt_num_correct_guesses = 0\n",
    "    for i in filtered_test_set:\n",
    "        guess = apply(filtered_weights, classes, filtered_test_set[i])\n",
    "        if guess == 1:\n",
    "            filtered_test_set[i].setLearnedClass(classes[1])\n",
    "            if filtered_test_set[i].getTrueClass() == filtered_test_set[i].getLearnedClass():\n",
    "                filt_num_correct_guesses += 1\n",
    "        if guess == 0:\n",
    "            filtered_test_set[i].setLearnedClass(classes[0])\n",
    "            if filtered_test_set[i].getTrueClass() == filtered_test_set[i].getLearnedClass():\n",
    "                filt_num_correct_guesses += 1\n",
    "\n",
    "    # Report accuracy\n",
    "    print (\"Learning constant: %.4f\" % float(learning_constant))\n",
    "    print (\"Number of iterations: %d\" % int(iterations))\n",
    "    print (\"Emails classified correctly: %d/%d\" % (num_correct_guesses, len(test_set)))\n",
    "    print (\"Accuracy: %.4f%%\" % (float(num_correct_guesses) / float(len(test_set)) * 100.0))\n",
    "    print (\"Filtered emails classified correctly: %d/%d\" % (filt_num_correct_guesses, len(filtered_test_set)))\n",
    "    print (\"Filtered accuracy: %.4f%%\" % (float(filt_num_correct_guesses) / float(len(filtered_test_set)) * 100.0))\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1651: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-53-0fa4b7987d85>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"data/dataset 1/train\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"data/dataset 1/test\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-52-18908ac8d01b>\u001B[0m in \u001B[0;36mmain\u001B[1;34m(train_dir, test_dir, iterations, learning_constant)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[1;31m# Set up data sets. Dictionaries containing the text, word frequencies, and true/learned classifications\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m     \u001B[0mmakeDataSet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraining_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dir\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/spam\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m     \u001B[0mmakeDataSet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraining_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dir\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/ham\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[0mmakeDataSet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_set\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_dir\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/spam\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-46-43a81def2557>\u001B[0m in \u001B[0;36mmakeDataSet\u001B[1;34m(storage_dict, directory, true_class)\u001B[0m\n\u001B[0;32m     10\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdir_entry_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtext_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m                 \u001B[1;31m# stores dictionary of dictionary of dictionary as explained above in the initialization\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m                 \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtext_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m                 \u001B[0mstorage_dict\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mdir_entry_path\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mDocument\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbagOfWords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrue_class\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\machine-learning-class\\lib\\encodings\\cp1252.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mIncrementalDecoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIncrementalDecoder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcharmap_decode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdecoding_table\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mStreamWriter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCodec\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStreamWriter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x9d in position 1651: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# main(\"data/dataset 1/train\", \"data/dataset 1/test\", 5, .5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}