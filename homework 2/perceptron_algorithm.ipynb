{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to execute\n",
    "\n",
    "#python perceptron_algorithm.py <training_directory> <testing_directory> <number_iterations> <learning_constant>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document class to store email instances\n",
    "class Document:\n",
    "    text = \"\"\n",
    "    word_freqs = {}\n",
    "\n",
    "    # spam or ham\n",
    "    true_class = \"\"\n",
    "    learned_class = \"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, text, counter, true_class):\n",
    "        self.text = text\n",
    "        self.word_freqs = counter\n",
    "        self.true_class = true_class\n",
    "\n",
    "    def getText(self):\n",
    "        return self.text\n",
    "\n",
    "    def getWordFreqs(self):\n",
    "        return self.word_freqs\n",
    "\n",
    "    def getTrueClass(self):\n",
    "        return self.true_class\n",
    "\n",
    "    def getLearnedClass(self):\n",
    "        return self.learned_class\n",
    "\n",
    "    def setLearnedClass(self, guess):\n",
    "        self.learned_class = guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts frequency of each word in the text files and order of sequence doesn't matter\n",
    "def bagOfWords(text):\n",
    "    bagsofwords = collections.Counter(re.findall(r'\\w+', text))\n",
    "    return dict(bagsofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all text files in given directory and construct the data set\n",
    "# the directory path should just be like \"train/ham\" for example\n",
    "# storage is the dictionary to store the email in\n",
    "# True class is the true classification of the email (spam or ham)\n",
    "def makeDataSet(storage_dict, directory, true_class):\n",
    "    for dir_entry in os.listdir(directory):\n",
    "        dir_entry_path = os.path.join(directory, dir_entry)\n",
    "        if os.path.isfile(dir_entry_path):\n",
    "            with open(dir_entry_path, 'r') as text_file:\n",
    "                # stores dictionary of dictionary of dictionary as explained above in the initialization\n",
    "                text = text_file.read()\n",
    "                storage_dict.update({dir_entry_path: Document(text, bagOfWords(text), true_class)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stop words\n",
    "def setStopWords(stop_word_text_file):\n",
    "    stops = []\n",
    "    with open(stop_word_text_file, 'r') as txt:\n",
    "        stops = (txt.read().splitlines())\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from data set and store in dictionary\n",
    "def removeStopWords(stops, data_set):\n",
    "    filtered_data_set = copy.deepcopy(data_set)\n",
    "    for i in stops:\n",
    "        for j in filtered_data_set:\n",
    "            if i in filtered_data_set[j].getWordFreqs():\n",
    "                del filtered_data_set[j].getWordFreqs()[i]\n",
    "    return filtered_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary of all the text in a data set\n",
    "def extractVocab(data_set):\n",
    "    v = []\n",
    "    for i in data_set:\n",
    "        for j in data_set[i].getWordFreqs():\n",
    "            if j not in v:\n",
    "                v.append(j)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learns weights using the perceptron training rule\n",
    "def learnWeights(weights, learning_constant, training_set, num_iterations, classes):\n",
    "    # Adjust weights num_iterations times\n",
    "    for i in num_iterations:\n",
    "        # Go through all training instances and update weights\n",
    "        for d in training_set:\n",
    "            # Used to get the current perceptron's output. If > 0, then spam, else output ham.\n",
    "            weight_sum = weights['weight_zero']\n",
    "            for f in training_set[d].getWordFreqs():\n",
    "                if f not in weights:\n",
    "                    weights[f] = 0.0\n",
    "                weight_sum += weights[f] * training_set[d].getWordFreqs()[f]\n",
    "            perceptron_output = 0.0\n",
    "            if weight_sum > 0:\n",
    "                perceptron_output = 1.0\n",
    "            target_value = 0.0\n",
    "            if training_set[d].getTrueClass() == classes[1]:\n",
    "                target_value = 1.0\n",
    "            # Update all weights that are relevant to the instance at hand\n",
    "            for w in training_set[d].getWordFreqs():\n",
    "                weights[w] += float(learning_constant) * float((target_value - perceptron_output)) * \\\n",
    "                                float(training_set[d].getWordFreqs()[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the algorithm to test accuracy on the test set. Returns the perceptron output\n",
    "def apply(weights, classes, instance):\n",
    "    weight_sum = weights['weight_zero']\n",
    "    for i in instance.getWordFreqs():\n",
    "        if i not in weights:\n",
    "            weights[i] = 0.0\n",
    "        weight_sum += weights[i] * instance.getWordFreqs()[i]\n",
    "    if weight_sum > 0:\n",
    "        # return is spam\n",
    "        return 1\n",
    "    else:\n",
    "        # return is ham\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes training directory containing spam and ham folder. Same with test directory\n",
    "# Also takes number of iterations and learning rate as parameters\n",
    "def main(train_dir, test_dir, iterations, learning_constant):\n",
    "    # Create dictionaries and lists needed\n",
    "    training_set = {}\n",
    "    test_set = {}\n",
    "    filtered_training_set = {}\n",
    "    filtered_test_set = {}\n",
    "\n",
    "    # Stop words to filter out\n",
    "    stop_words = setStopWords('stop_words.txt')\n",
    "\n",
    "    # ham = 0 for not spam, spam = 1 for is spam\n",
    "    classes = [\"ham\", \"spam\"]\n",
    "\n",
    "    # Number of iterations and learning constant (usually around .1 or .01)\n",
    "    iterations = iterations\n",
    "    learning_constant = learning_constant\n",
    "\n",
    "    # Set up data sets. Dictionaries containing the text, word frequencies, and true/learned classifications\n",
    "    makeDataSet(training_set, train_dir + \"/spam\", classes[1])\n",
    "    makeDataSet(training_set, train_dir + \"/ham\", classes[0])\n",
    "    makeDataSet(test_set, test_dir + \"/spam\", classes[1])\n",
    "    makeDataSet(test_set, test_dir + \"/ham\", classes[0])\n",
    "\n",
    "    # Set up data sets without stop words\n",
    "    filtered_training_set = removeStopWords(stop_words, training_set)\n",
    "    filtered_test_set = removeStopWords(stop_words, test_set)\n",
    "\n",
    "    # Extract training set vocabulary\n",
    "    training_set_vocab = extractVocab(training_set)\n",
    "    filtered_training_set_vocab = extractVocab(filtered_training_set)\n",
    "\n",
    "    # store weights as dictionary. w0 initiall 1.0, others initially 1.0. token : weight value\n",
    "    weights = {'weight_zero': 1.0}\n",
    "    filtered_weights = {'weight_zero': 1.0}\n",
    "    for i in training_set_vocab:\n",
    "        weights[i] = 0.0\n",
    "    for i in filtered_training_set_vocab:\n",
    "        filtered_weights[i] = 0.0\n",
    "\n",
    "    # Learn weights using the training_set and the filtered_training_set\n",
    "    learnWeights(weights, learning_constant, training_set, iterations, classes)\n",
    "    learnWeights(filtered_weights, learning_constant, filtered_training_set, iterations, classes)\n",
    "\n",
    "    #Apply the algorithm on the test set and report accuracy\n",
    "    num_correct_guesses = 0\n",
    "    for i in test_set:\n",
    "        guess = apply(weights, classes, test_set[i])\n",
    "        if guess == 1:\n",
    "            test_set[i].setLearnedClass(classes[1])\n",
    "            if test_set[i].getTrueClass() == test_set[i].getLearnedClass():\n",
    "                num_correct_guesses += 1\n",
    "        if guess == 0:\n",
    "            test_set[i].setLearnedClass(classes[0])\n",
    "            if test_set[i].getTrueClass() == test_set[i].getLearnedClass():\n",
    "                num_correct_guesses += 1\n",
    "\n",
    "    # Apply algorithm again on test set without any stop words and report accuracy\n",
    "    filt_num_correct_guesses = 0\n",
    "    for i in filtered_test_set:\n",
    "        guess = apply(filtered_weights, classes, filtered_test_set[i])\n",
    "        if guess == 1:\n",
    "            filtered_test_set[i].setLearnedClass(classes[1])\n",
    "            if filtered_test_set[i].getTrueClass() == filtered_test_set[i].getLearnedClass():\n",
    "                filt_num_correct_guesses += 1\n",
    "        if guess == 0:\n",
    "            filtered_test_set[i].setLearnedClass(classes[0])\n",
    "            if filtered_test_set[i].getTrueClass() == filtered_test_set[i].getLearnedClass():\n",
    "                filt_num_correct_guesses += 1\n",
    "\n",
    "    # Report accuracy\n",
    "    print (\"Learning constant: %.4f\" % float(learning_constant))\n",
    "    print (\"Number of iterations: %d\" % int(iterations))\n",
    "    print (\"Emails classified correctly: %d/%d\" % (num_correct_guesses, len(test_set)))\n",
    "    print (\"Accuracy: %.4f%%\" % (float(num_correct_guesses) / float(len(test_set)) * 100.0))\n",
    "    print (\"Filtered emails classified correctly: %d/%d\" % (filt_num_correct_guesses, len(filtered_test_set)))\n",
    "    print (\"Filtered accuracy: %.4f%%\" % (float(filt_num_correct_guesses) / float(len(filtered_test_set)) * 100.0))\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
