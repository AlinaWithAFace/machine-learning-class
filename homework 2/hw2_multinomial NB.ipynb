{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores emails as dictionaries. email_file_name : Document (class defined below)\n",
    "training_set = dict()\n",
    "test_set = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered sets without stop words\n",
    "filtered_training_set = dict()\n",
    "filtered_test_set = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of Stop words\n",
    "stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham = 0 for not spam, spam = 1 for is spam\n",
    "classes = [\"ham\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional probability from the training data\n",
    "conditional_probability = dict()\n",
    "filtered_conditional_probability = dict()\n",
    "# Prior for the classifications using the training data\n",
    "prior = dict()\n",
    "filtered_prior = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all text files in the given directory and construct the data set, D\n",
    "# the directory path should just be like \"train/ham\" for example\n",
    "# storage is the dictionary to store the email in\n",
    "# True class is the true classification of the email (spam or ham)\n",
    "def makeDataSet(storage_dict, directory, true_class):\n",
    "    for dir_entry in os.listdir(directory):\n",
    "        dir_entry_path = os.path.join(directory, dir_entry)\n",
    "        if os.path.isfile(dir_entry_path):\n",
    "            with open(dir_entry_path, 'r') as text_file:\n",
    "                # stores dictionary of dictionary of dictionary as explained above in the initialization\n",
    "                text = text_file.read()\n",
    "                storage_dict.update({dir_entry_path: Document(text, bagOfWords(text), true_class)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts frequency of each word in the text files and order of sequence doesn't matter\n",
    "def bagOfWords(text):\n",
    "    bagsofwords = collections.Counter(re.findall(r'\\w+', text))\n",
    "    return dict(bagsofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary of all the text in a data set\n",
    "def extractVocab(data_set):\n",
    "    all_text = \"\"\n",
    "    v = []\n",
    "    for x in data_set:\n",
    "        all_text += data_set[x].getText()\n",
    "    for y in bagOfWords(all_text):\n",
    "        v.append(y)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stop words\n",
    "def setStopWords():\n",
    "    stops = []\n",
    "    with open('stop_words.txt', 'r') as txt:\n",
    "        stops = (txt.read().splitlines())\n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from data set and store in dictionary\n",
    "def removeStopWords(stops, data_set):\n",
    "    filtered_data_set = copy.deepcopy(data_set)\n",
    "    for i in stops:\n",
    "        for j in filtered_data_set:\n",
    "            if i in filtered_data_set[j].getWordFreqs():\n",
    "                del filtered_data_set[j].getWordFreqs()[i]\n",
    "    return filtered_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def trainMultinomialNB(training, priors, cond):\n",
    "    # v is the vocabulary of the training set\n",
    "    v = extractVocab(training)\n",
    "    # n is the number of documents\n",
    "    n = len(training)\n",
    "    # for each class in classes (i.e. ham and spam)\n",
    "    for c in classes:\n",
    "        # n_c is number of documents with true class c\n",
    "        n_c = 0.0\n",
    "        # text_c = concatenation of text of all docs in class (D, c)\n",
    "        text_c = \"\"\n",
    "        for i in training:\n",
    "            if training[i].getTrueClass() == c:\n",
    "                n_c += 1\n",
    "                text_c += training[i].getText()\n",
    "        priors[c] = float(n_c) / float(n)\n",
    "        # Count frequencies/tokens of each term in text_c in dictionary form (i.e. token : frequency)\n",
    "        token_freqs = bagOfWords(text_c)\n",
    "        # Calculate conditional probabilities for each token and sum using laplace smoothing and log-scale\n",
    "        for t in v:\n",
    "            if t in token_freqs:\n",
    "                cond.update({t + \"_\" + c: (float((token_freqs[t] + 1.0)) / float((len(text_c) + len(token_freqs))))})\n",
    "            else:\n",
    "                cond.update({t + \"_\" + c: (float(1.0) / float((len(text_c) + len(token_freqs))))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing. Data instance is a Document\n",
    "# Returns classification guess\n",
    "def applyMultinomialNB(data_instance, priors, cond):\n",
    "    score = {}\n",
    "    for c in classes:\n",
    "        score[c] = math.log10(float(priors[c]))\n",
    "        for t in data_instance.getWordFreqs():\n",
    "            if (t + \"_\" + c) in cond:\n",
    "                score[c] += float(math.log10(cond[t + \"_\" + c]))\n",
    "    if score[\"spam\"] > score[\"ham\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document class to store email instances easier\n",
    "class Document:\n",
    "    text = \"\"\n",
    "    word_freqs = {}\n",
    "\n",
    "    # spam or ham\n",
    "    true_class = \"\"\n",
    "    learned_class = \"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, text, counter, true_class):\n",
    "        self.text = text\n",
    "        self.word_freqs = counter\n",
    "        self.true_class = true_class\n",
    "\n",
    "    def getText(self):\n",
    "        return self.text\n",
    "\n",
    "    def getWordFreqs(self):\n",
    "        return self.word_freqs\n",
    "\n",
    "    def getTrueClass(self):\n",
    "        return self.true_class\n",
    "\n",
    "    def getLearnedClass(self):\n",
    "        return self.learned_class\n",
    "\n",
    "    def setLearnedClass(self, guess):\n",
    "        self.learned_class = guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes directories holding the data text files as paramters. \"train/ham\" for example\n",
    "def main(training_spam_dir, training_ham_dir, test_spam_dir, test_ham_dir):\n",
    "    # Set up data sets. Dictionaries containing the text, word frequencies, and true/learned classifications\n",
    "    makeDataSet(training_set, training_spam_dir, classes[1])\n",
    "    makeDataSet(training_set, training_ham_dir, classes[0])\n",
    "    makeDataSet(test_set, test_spam_dir, classes[1])\n",
    "    makeDataSet(test_set, test_ham_dir, classes[0])\n",
    "\n",
    "    # Set the stop words list\n",
    "    stop_words = setStopWords()\n",
    "\n",
    "    # Set up data sets without stop words\n",
    "    filtered_training_set = removeStopWords(stop_words, training_set)\n",
    "    filtered_test_set = removeStopWords(stop_words, test_set)\n",
    "\n",
    "    # Train using the training data\n",
    "    trainMultinomialNB(training_set, prior, conditional_probability)\n",
    "    trainMultinomialNB(filtered_training_set, filtered_prior, filtered_conditional_probability)\n",
    "\n",
    "    # Test using the testing data - unfiltered\n",
    "    correct_guesses = 0\n",
    "    for i in test_set:\n",
    "        test_set[i].setLearnedClass(applyMultinomialNB(test_set[i], prior, conditional_probability))\n",
    "        if test_set[i].getLearnedClass() == test_set[i].getTrueClass():\n",
    "            correct_guesses += 1\n",
    "\n",
    "    # Test using the testing data - filtered\n",
    "    correct_guesses_filtered = 0\n",
    "    for i in filtered_test_set:\n",
    "        filtered_test_set[i].setLearnedClass(applyMultinomialNB(filtered_test_set[i], filtered_prior,\n",
    "                                                                filtered_conditional_probability))\n",
    "        if filtered_test_set[i].getLearnedClass() == filtered_test_set[i].getTrueClass():\n",
    "            correct_guesses_filtered += 1\n",
    "\n",
    "    print(\"Correct guesses before filtering stop words:\\t%d/%s\") % (correct_guesses, len(test_set))\n",
    "    print(\"Accuracy before filtering stop words:\\t\\t\\t%.4f%%\") % (100.0 * float(correct_guesses) / float(len(test_set)))\n",
    "    print(\"Correct guesses after filtering stop words:\\t\\t %d / %s\") % (correct_guesses_filtered, len(filtered_test_set))\n",
    "    print(\"Accuracy after filtering stop words:\\t\\t\\t%.4f%%\") % (100.0 * float(correct_guesses_filtered) / float(len(filtered_test_set)))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
