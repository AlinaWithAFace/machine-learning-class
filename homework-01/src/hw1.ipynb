{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XO = 0 : \n",
      "| XM = 0 : \n",
      "| | XF = 0 : \n",
      "| | | XB = 0 : \n",
      "| | | | XG = 0 : 1 \n",
      "| | | | XG = 1 : \n",
      "| | | | | XD = 0 : \n",
      "| | | | | | XS = 0 : 1 \n",
      "| | | | | | XS = 1 : \n",
      "| | | | | | | XC = 0 : 0 \n",
      "| | | | | | | XC = 1 : \n",
      "| | | | | | | | XH = 0 : 1 \n",
      "| | | | | | | | XH = 1 : 0 \n",
      "| | | | | XD = 1 : \n",
      "| | | | | | XP = 0 : \n",
      "| | | | | | | XE = 0 : 1 \n",
      "| | | | | | | XE = 1 : 0 \n",
      "| | | | | | XP = 1 : 1 \n",
      "| | | XB = 1 : \n",
      "| | | | XD = 0 : 1 \n",
      "| | | | XD = 1 : \n",
      "| | | | | XI = 0 : 1 \n",
      "| | | | | XI = 1 : \n",
      "| | | | | | XG = 0 : 0 \n",
      "| | | | | | XG = 1 : 1 \n",
      "| | XF = 1 : 1 \n",
      "| XM = 1 : \n",
      "| | XB = 0 : \n",
      "| | | XD = 0 : \n",
      "| | | | XG = 0 : \n",
      "| | | | | XF = 0 : 1 \n",
      "| | | | | XF = 1 : \n",
      "| | | | | | XJ = 0 : \n",
      "| | | | | | | XN = 0 : 0 \n",
      "| | | | | | | XN = 1 : \n",
      "| | | | | | | | XE = 0 : \n",
      "| | | | | | | | | XK = 0 : 1 \n",
      "| | | | | | | | | XK = 1 : 0 \n",
      "| | | | | | | | XE = 1 : 1 \n",
      "| | | | | | XJ = 1 : \n",
      "| | | | | | | XC = 0 : \n",
      "| | | | | | | | XT = 0 : \n",
      "| | | | | | | | | XL = 0 : \n",
      "| | | | | | | | | | XE = 0 : \n",
      "| | | | | | | | | | | XI = 0 : 1 \n",
      "| | | | | | | | | | | XI = 1 : 0 \n",
      "| | | | | | | | | | XE = 1 : 0 \n",
      "| | | | | | | | | XL = 1 : 1 \n",
      "| | | | | | | | XT = 1 : 0 \n",
      "| | | | | | | XC = 1 : 0 \n",
      "| | | | XG = 1 : \n",
      "| | | | | XU = 0 : 0 \n",
      "| | | | | XU = 1 : \n",
      "| | | | | | XI = 0 : 1 \n",
      "| | | | | | XI = 1 : 0 \n",
      "| | | XD = 1 : \n",
      "| | | | XC = 0 : \n",
      "| | | | | XF = 0 : \n",
      "| | | | | | XG = 0 : 1 \n",
      "| | | | | | XG = 1 : \n",
      "| | | | | | | XP = 0 : \n",
      "| | | | | | | | XS = 0 : 1 \n",
      "| | | | | | | | XS = 1 : 0 \n",
      "| | | | | | | XP = 1 : 1 \n",
      "| | | | | XF = 1 : \n",
      "| | | | | | XG = 0 : \n",
      "| | | | | | | XP = 0 : 0 \n",
      "| | | | | | | XP = 1 : \n",
      "| | | | | | | | XL = 0 : 1 \n",
      "| | | | | | | | XL = 1 : 0 \n",
      "| | | | | | XG = 1 : \n",
      "| | | | | | | XQ = 0 : 1 \n",
      "| | | | | | | XQ = 1 : \n",
      "| | | | | | | | XR = 0 : 0 \n",
      "| | | | | | | | XR = 1 : 1 \n",
      "| | | | XC = 1 : 1 \n",
      "| | XB = 1 : \n",
      "| | | XI = 0 : 1 \n",
      "| | | XI = 1 : \n",
      "| | | | XC = 0 : \n",
      "| | | | | XK = 0 : \n",
      "| | | | | | XP = 0 : 0 \n",
      "| | | | | | XP = 1 : \n",
      "| | | | | | | XS = 0 : \n",
      "| | | | | | | | XG = 0 : 0 \n",
      "| | | | | | | | XG = 1 : \n",
      "| | | | | | | | | XF = 0 : 1 \n",
      "| | | | | | | | | XF = 1 : 0 \n",
      "| | | | | | | XS = 1 : 1 \n",
      "| | | | | XK = 1 : 1 \n",
      "| | | | XC = 1 : 1 \n",
      "XO = 1 : \n",
      "| XI = 0 : \n",
      "| | XM = 0 : \n",
      "| | | XQ = 0 : \n",
      "| | | | XF = 0 : \n",
      "| | | | | XH = 0 : \n",
      "| | | | | | XB = 0 : 1 \n",
      "| | | | | | XB = 1 : \n",
      "| | | | | | | XC = 0 : 0 \n",
      "| | | | | | | XC = 1 : 1 \n",
      "| | | | | XH = 1 : 0 \n",
      "| | | | XF = 1 : 1 \n",
      "| | | XQ = 1 : \n",
      "| | | | XJ = 0 : \n",
      "| | | | | XN = 0 : \n",
      "| | | | | | XP = 0 : 0 \n",
      "| | | | | | XP = 1 : \n",
      "| | | | | | | XB = 0 : \n",
      "| | | | | | | | XF = 0 : 1 \n",
      "| | | | | | | | XF = 1 : 0 \n",
      "| | | | | | | XB = 1 : 1 \n",
      "| | | | | XN = 1 : 1 \n",
      "| | | | XJ = 1 : \n",
      "| | | | | XH = 0 : \n",
      "| | | | | | XL = 0 : 1 \n",
      "| | | | | | XL = 1 : 0 \n",
      "| | | | | XH = 1 : \n",
      "| | | | | | XU = 0 : 0 \n",
      "| | | | | | XU = 1 : \n",
      "| | | | | | | XE = 0 : 0 \n",
      "| | | | | | | XE = 1 : 1 \n",
      "| | XM = 1 : \n",
      "| | | XQ = 0 : \n",
      "| | | | XF = 0 : \n",
      "| | | | | XL = 0 : \n",
      "| | | | | | XC = 0 : 0 \n",
      "| | | | | | XC = 1 : \n",
      "| | | | | | | XH = 0 : 0 \n",
      "| | | | | | | XH = 1 : \n",
      "| | | | | | | | XU = 0 : \n",
      "| | | | | | | | | XB = 0 : \n",
      "| | | | | | | | | | XD = 0 : 0 \n",
      "| | | | | | | | | | XD = 1 : 1 \n",
      "| | | | | | | | | XB = 1 : 1 \n",
      "| | | | | | | | XU = 1 : 0 \n",
      "| | | | | XL = 1 : \n",
      "| | | | | | XC = 0 : \n",
      "| | | | | | | XB = 0 : 1 \n",
      "| | | | | | | XB = 1 : \n",
      "| | | | | | | | XP = 0 : 1 \n",
      "| | | | | | | | XP = 1 : 0 \n",
      "| | | | | | XC = 1 : 0 \n",
      "| | | | XF = 1 : 1 \n",
      "| | | XQ = 1 : 1 \n",
      "| XI = 1 : \n",
      "| | XT = 0 : \n",
      "| | | XH = 0 : \n",
      "| | | | XP = 0 : \n",
      "| | | | | XF = 0 : 1 \n",
      "| | | | | XF = 1 : \n",
      "| | | | | | XQ = 0 : \n",
      "| | | | | | | XK = 0 : 0 \n",
      "| | | | | | | XK = 1 : \n",
      "| | | | | | | | XC = 0 : 1 \n",
      "| | | | | | | | XC = 1 : 0 \n",
      "| | | | | | XQ = 1 : \n",
      "| | | | | | | XK = 0 : 1 \n",
      "| | | | | | | XK = 1 : 0 \n",
      "| | | | XP = 1 : \n",
      "| | | | | XS = 0 : \n",
      "| | | | | | XD = 0 : \n",
      "| | | | | | | XC = 0 : \n",
      "| | | | | | | | XJ = 0 : \n",
      "| | | | | | | | | XN = 0 : 1 \n",
      "| | | | | | | | | XN = 1 : 0 \n",
      "| | | | | | | | XJ = 1 : \n",
      "| | | | | | | | | XG = 0 : \n",
      "| | | | | | | | | | XB = 0 : 1 \n",
      "| | | | | | | | | | XB = 1 : 0 \n",
      "| | | | | | | | | XG = 1 : 1 \n",
      "| | | | | | | XC = 1 : 1 \n",
      "| | | | | | XD = 1 : \n",
      "| | | | | | | XK = 0 : \n",
      "| | | | | | | | XF = 0 : \n",
      "| | | | | | | | | XC = 0 : 0 \n",
      "| | | | | | | | | XC = 1 : 1 \n",
      "| | | | | | | | XF = 1 : 0 \n",
      "| | | | | | | XK = 1 : 1 \n",
      "| | | | | XS = 1 : 0 \n",
      "| | | XH = 1 : \n",
      "| | | | XJ = 0 : \n",
      "| | | | | XC = 0 : \n",
      "| | | | | | XN = 0 : 0 \n",
      "| | | | | | XN = 1 : \n",
      "| | | | | | | XF = 0 : \n",
      "| | | | | | | | XG = 0 : 0 \n",
      "| | | | | | | | XG = 1 : 1 \n",
      "| | | | | | | XF = 1 : 1 \n",
      "| | | | | XC = 1 : \n",
      "| | | | | | XM = 0 : 1 \n",
      "| | | | | | XM = 1 : \n",
      "| | | | | | | XF = 0 : \n",
      "| | | | | | | | XR = 0 : 0 \n",
      "| | | | | | | | XR = 1 : 1 \n",
      "| | | | | | | XF = 1 : 0 \n",
      "| | | | XJ = 1 : \n",
      "| | | | | XS = 0 : 0 \n",
      "| | | | | XS = 1 : \n",
      "| | | | | | XG = 0 : \n",
      "| | | | | | | XB = 0 : 1 \n",
      "| | | | | | | XB = 1 : \n",
      "| | | | | | | | XD = 0 : 0 \n",
      "| | | | | | | | XD = 1 : \n",
      "| | | | | | | | | XE = 0 : 1 \n",
      "| | | | | | | | | XE = 1 : 0 \n",
      "| | | | | | XG = 1 : \n",
      "| | | | | | | XC = 0 : 0 \n",
      "| | | | | | | XC = 1 : \n",
      "| | | | | | | | XD = 0 : 0 \n",
      "| | | | | | | | XD = 1 : 1 \n",
      "| | XT = 1 : \n",
      "| | | XS = 0 : \n",
      "| | | | XQ = 0 : \n",
      "| | | | | XK = 0 : \n",
      "| | | | | | XC = 0 : \n",
      "| | | | | | | XR = 0 : \n",
      "| | | | | | | | XH = 0 : \n",
      "| | | | | | | | | XE = 0 : 1 \n",
      "| | | | | | | | | XE = 1 : 0 \n",
      "| | | | | | | | XH = 1 : 0 \n",
      "| | | | | | | XR = 1 : \n",
      "| | | | | | | | XB = 0 : \n",
      "| | | | | | | | | XD = 0 : 1 \n",
      "| | | | | | | | | XD = 1 : 0 \n",
      "| | | | | | | | XB = 1 : 1 \n",
      "| | | | | | XC = 1 : 0 \n",
      "| | | | | XK = 1 : \n",
      "| | | | | | XD = 0 : \n",
      "| | | | | | | XF = 0 : 1 \n",
      "| | | | | | | XF = 1 : 0 \n",
      "| | | | | | XD = 1 : 1 \n",
      "| | | | XQ = 1 : \n",
      "| | | | | XM = 0 : \n",
      "| | | | | | XN = 0 : \n",
      "| | | | | | | XU = 0 : 0 \n",
      "| | | | | | | XU = 1 : 1 \n",
      "| | | | | | XN = 1 : \n",
      "| | | | | | | XP = 0 : \n",
      "| | | | | | | | XB = 0 : \n",
      "| | | | | | | | | XF = 0 : 1 \n",
      "| | | | | | | | | XF = 1 : 0 \n",
      "| | | | | | | | XB = 1 : 0 \n",
      "| | | | | | | XP = 1 : 0 \n",
      "| | | | | XM = 1 : 1 \n",
      "| | | XS = 1 : \n",
      "| | | | XL = 0 : \n",
      "| | | | | XD = 0 : \n",
      "| | | | | | XU = 0 : 0 \n",
      "| | | | | | XU = 1 : \n",
      "| | | | | | | XB = 0 : \n",
      "| | | | | | | | XE = 0 : 0 \n",
      "| | | | | | | | XE = 1 : \n",
      "| | | | | | | | | XC = 0 : 0 \n",
      "| | | | | | | | | XC = 1 : 1 \n",
      "| | | | | | | XB = 1 : \n",
      "| | | | | | | | XG = 0 : \n",
      "| | | | | | | | | XH = 0 : 1 \n",
      "| | | | | | | | | XH = 1 : 0 \n",
      "| | | | | | | | XG = 1 : 1 \n",
      "| | | | | XD = 1 : \n",
      "| | | | | | XG = 0 : 1 \n",
      "| | | | | | XG = 1 : 0 \n",
      "| | | | XL = 1 : \n",
      "| | | | | XE = 0 : \n",
      "| | | | | | XK = 0 : \n",
      "| | | | | | | XG = 0 : 0 \n",
      "| | | | | | | XG = 1 : \n",
      "| | | | | | | | XP = 0 : 1 \n",
      "| | | | | | | | XP = 1 : 0 \n",
      "| | | | | | XK = 1 : 1 \n",
      "| | | | | XE = 1 : \n",
      "| | | | | | XM = 0 : \n",
      "| | | | | | | XG = 0 : 1 \n",
      "| | | | | | | XG = 1 : 0 \n",
      "| | | | | | XM = 1 : 1 Accuracy with IG3 75.85\n",
      "Accuracy with Variance Impuruty 76.65\n",
      "Accuracy with pruned Information gain tree 77.4\n",
      "Accuracy with pruned Variance gain tree 76.65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "L = int(sys.argv[1])\n",
    "K = int(sys.argv[2])\n",
    "training_set_path = sys.argv[3]\n",
    "validation_set_path = sys.argv[4]\n",
    "test_set_path = sys.argv[5]\n",
    "to_print_input = sys.argv[6]\n",
    "\n",
    "training_set = pd.read_csv(training_set_path)\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "validation_set = pd.read_csv(validation_set_path)\n",
    "\n",
    "if to_print_input == \"yes\":\n",
    "    to_print = True\n",
    "elif to_print_input == \"no\":\n",
    "    to_print = False\n",
    "else:\n",
    "    print(\"to-print must be 'yes' or 'no', printing anyway\")\n",
    "to_print = True\n",
    "\n",
    "def node_formation(df):\n",
    "    \"\"\"\n",
    "    recursive function which drills down until a pure node is returned\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Check current variance\n",
    "    variance = variance_impurity(df)\n",
    "\n",
    "    if variance == 0:\n",
    "        # Pure node\n",
    "        if (df['Class'] == 0).sum() == 0:\n",
    "            class_outcome = 0\n",
    "        else:\n",
    "            class_outcome = 1\n",
    "\n",
    "        return {'Class': class_outcome}\n",
    "\n",
    "    else:\n",
    "        node_attribute, zero, one = gain(df)  # returns attribute to split on, and if zero or ones were present\n",
    "        if zero:\n",
    "            branch_0 = node_formation(df[df[node_attribute] == 0].drop([node_attribute], axis=1))\n",
    "            # filters df by attribute value and drops the named attribute\n",
    "        if one:\n",
    "            branch_1 = node_formation(df[df[node_attribute] == 1].drop([node_attribute], axis=1))\n",
    "\n",
    "        if zero and one:  # attribute contained both\n",
    "            return {node_attribute: {0: branch_0, 1: branch_1}}\n",
    "        if zero:\n",
    "            return {node_attribute: {0: branch_0}}\n",
    "        if one:\n",
    "            return {node_attribute: {1: branch_1}}\n",
    "\n",
    "\n",
    "def variance_impurity(df):\n",
    "    \"\"\"\n",
    "    Input is dataframe, which may be subset of larger set\n",
    "    VI(S) = (K0/K*K1/K)\n",
    "    K0, class = 0\n",
    "    K1, class = 1\n",
    "    K = sum(K0 + K)\n",
    "    Returns variance impurity\n",
    "    \"\"\"\n",
    "\n",
    "    K0 = (df['Class'] == 0).sum()  # Turns df in True/False\n",
    "    K1 = (df['Class'] == 1).sum()\n",
    "    K = K0 + K1\n",
    "\n",
    "    if K == 0:\n",
    "        # print(\"K is 0, returning 0\")\n",
    "        return 0\n",
    "\n",
    "    K2 = K0 / K\n",
    "    # print(\"K2:\")\n",
    "    # print(K2)\n",
    "    K3 = K1 / K\n",
    "    # print(\"K3:\")\n",
    "    # print(K3)\n",
    "\n",
    "    variance = (K2 * K3)\n",
    "    # print(\"variance:\")\n",
    "    # print(variance)\n",
    "\n",
    "    return variance\n",
    "\n",
    "\n",
    "def count_01s(df):\n",
    "    \"\"\"\n",
    "    Input is dataframe, which may be subset of larger set\n",
    "    Creates a dictionary for each remaining attributes/elements in the format\n",
    "    {attribute: (zeros, ones)}\n",
    "    returns dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    attribute_counts = {}\n",
    "    # dictionary for attributes with counts as tuples (0, 1)\n",
    "    # This creates the elements, counts, feel free to pop out\n",
    "    for attribute in df.loc[:, df.columns != ('Class' or '' or 'None')]:\n",
    "        # loops over all attributes not identified as Class, the outcome\n",
    "        zeros = (df[attribute] == 0).sum()  # Turns df in True/False\n",
    "        ones = (df[attribute] == 1).sum()\n",
    "        attribute_counts[attribute] = (zeros, ones)\n",
    "\n",
    "    return attribute_counts\n",
    "\n",
    "\n",
    "def gain(df):\n",
    "    \"\"\"\n",
    "    Input df\n",
    "    variance_impurity and count_01 in finding the maximum gain\n",
    "    Returns the attribute/element which the highest gain\n",
    "    \"\"\"\n",
    "\n",
    "    target_values = count_01s(df)\n",
    "    VIS = variance_impurity(df)\n",
    "    initialize = True\n",
    "\n",
    "    for attribute in target_values:\n",
    "        zeros = target_values[attribute][0]\n",
    "        ones = target_values[attribute][1]\n",
    "        Pr0 = zeros / (zeros + ones)\n",
    "        Pr1 = 1 - Pr0  # Using axioms, probability will sum to 1\n",
    "        VIS0 = variance_impurity(df[df[attribute] == 0])\n",
    "        VIS1 = variance_impurity(df[df[attribute] == 1])\n",
    "        gainSX = VIS - Pr0 * VIS0 - Pr1 * VIS1\n",
    "\n",
    "        if initialize:\n",
    "            max_gain = (attribute, gainSX)\n",
    "            initialize = False\n",
    "        if gainSX > max_gain[1]:\n",
    "            max_gain = (attribute, gainSX)\n",
    "\n",
    "    attribute = max_gain[0]\n",
    "    zero, one = False, False\n",
    "    if target_values[attribute][0] > 0:\n",
    "        zero = True\n",
    "    if target_values[attribute][1] > 0:\n",
    "        one = True\n",
    "\n",
    "    return attribute, zero, one\n",
    "\n",
    "def print_decision_tree(layer_count, dictionary):\n",
    "    \"\"\"\n",
    "    Recursively go through the dictionaries and print out the keys/values, adding space to every layer for a tiered visual\n",
    "    :param layer_count: the depth of the layer, initially use -1, gets added to every recursion\n",
    "    :param dictionary: the dictionary to print, might not actually be a dictionary if we're at the end of the layers\n",
    "    :return: a printed decision tree\n",
    "    \"\"\"\n",
    "    if type(dictionary) != dict:\n",
    "        # print(\"dictionary not a dictionary, return\")\n",
    "        return\n",
    "    layer_count = layer_count + 1\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "\n",
    "        # for i in range(0, layer_count): print(\"| \", end=\"\")\n",
    "\n",
    "        # print(\"key {} \".format(key))\n",
    "\n",
    "        if type(value) == dict:\n",
    "            for v in value:\n",
    "                print(\"\")\n",
    "                for i in range(0, layer_count): print(\"| \", end=\"\")\n",
    "                print(\"{} = {} : \".format(key, v), end=\"\")\n",
    "\n",
    "                print_decision_tree(layer_count, value[v])\n",
    "        else:\n",
    "            # for i in range(0, layer_count): print(\"| \", end=\"\")\n",
    "            # print(\"value not a dictionary, end of tree?\")\n",
    "            print(\"{} \".format(value), end=\"\")\n",
    "\n",
    "\n",
    "node_dict = node_formation(training_set)\n",
    "\n",
    "if to_print:\n",
    "    print_decision_tree(-1, node_dict)\n",
    "\n",
    "#I create two dictionaries: one for the Variance Impurity Tree and one for the IG3 Tree\n",
    "n = 0\n",
    "node_position = 0\n",
    "node_variance = 0\n",
    "\n",
    "labelValues = list(training_set.columns.values)\n",
    "labelValues.remove('Class')\n",
    "\n",
    "#I create two dictionaries: one for the Variance Impurity Tree and one for the IG3 Tree\n",
    "def counts_in_list(seq, return_counts=False, id=None):\n",
    "   \n",
    "    found = set()\n",
    "    if id is None:\n",
    "        for x in seq:\n",
    "            found.add(x)\n",
    "           \n",
    "    else:\n",
    "        for x in seq:\n",
    "            x = id(x)\n",
    "            if x not in found:\n",
    "                found.add(x)\n",
    "    found = list(found)           \n",
    "    counts = [seq.count(0),seq.count(1)]\n",
    "    if return_counts:\n",
    "        return found,counts\n",
    "    else:\n",
    "        return found\n",
    "\n",
    "#start with variance impurity tree\n",
    "def calculate_variance(target_values):\n",
    "    values = list(target_values)\n",
    "    elements,counts = counts_in_list(values,True)\n",
    "    variance_impurity = 0\n",
    "    sum_counts = sum(counts)\n",
    "    for i in elements:\n",
    "        variance_impurity += (-counts[i]/sum_counts*(counts[i]/sum_counts))\n",
    "    return variance_impurity\n",
    "\n",
    "def variance_impurity_gain(data, split_attribute_name, target_attribute_name):\n",
    "    data_split = data.groupby(split_attribute_name)\n",
    "    data_subgroup = data_split.agg({target_attribute_name : [calculate_variance, lambda x: len(x)/(len(data.index) * 1.0)] })[target_attribute_name]\n",
    "    data_subgroup.columns = ['Variance', 'Observations']\n",
    "    weighted_variance_impurity = sum( data_subgroup['Variance'] * data_subgroup['Observations'] )\n",
    "    total_variance_impurity = calculate_variance(data[target_attribute_name])\n",
    "    variance_impurity_gain = total_variance_impurity - weighted_variance_impurity\n",
    "    return variance_impurity_gain\n",
    "\n",
    "def tree_with_variance_impurity_algorithm(data, target_attribute_name, attribute_names, default_class=None):\n",
    "    global node_variance\n",
    "    from collections import Counter\n",
    "    count_target_attributes = Counter(x for x in data[target_attribute_name])\n",
    "    if len(count_target_attributes) == 1:\n",
    "        return list(count_target_attributes.keys())[0]\n",
    "\n",
    "    elif data.empty or (not attribute_names):\n",
    "        return default_class \n",
    "    \n",
    "    else:\n",
    "        index_of_max = list(count_target_attributes.values()).index(max(count_target_attributes.values())) \n",
    "        default_class = list(count_target_attributes.keys())[index_of_max]\n",
    "        variance_gain = [variance_impurity_gain(data, attr, target_attribute_name) for attr in attribute_names]\n",
    "        index_of_max = variance_gain.index(max(variance_gain)) \n",
    "        best_attr = attribute_names[index_of_max]\n",
    "        \n",
    "        tree = {best_attr:{}}\n",
    "        positiveCount = data['Class'].value_counts()[1];\n",
    "        negativeCount = data['Class'].value_counts()[0];\n",
    "        if positiveCount>negativeCount :\n",
    "            best_class = 1\n",
    "        elif positiveCount<negativeCount:\n",
    "            best_class = 0\n",
    "        else:\n",
    "            best_class = 'none'\n",
    "        tree[best_attr]['best_class'] = best_class\n",
    "        node_variance = node_variance + 1\n",
    "        tree[best_attr]['number'] = node_variance\n",
    "        remaining_attribute_names = [i for i in attribute_names if i != best_attr]\n",
    "\n",
    "        for attr_val, data_subset in data.groupby(best_attr):\n",
    "            subtree = tree_with_variance_impurity_algorithm(data_subset,\n",
    "                        target_attribute_name,\n",
    "                        remaining_attribute_names,\n",
    "                        default_class)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree\n",
    "\n",
    "#now I create a dictionary for the IG3 Tree    \n",
    "def calculate_entropy(probablities):\n",
    "    import math\n",
    "    sum_of_probablities = 0;\n",
    "    for prob in probablities:\n",
    "        sum_of_probablities += -prob*math.log(prob, 2)\n",
    "    return sum_of_probablities\n",
    "\n",
    "def calculate_entropy_of_the_list(list):\n",
    "    from collections import Counter  \n",
    "    counter = Counter(x for x in list)\n",
    "    num_instances = len(list)*1.0\n",
    "    probs = [x / num_instances for x in counter.values()]\n",
    "    return calculate_entropy(probs)\n",
    "    \n",
    "def information_gain(data, split_attribute_name, target_attribute_name):\n",
    "    data_split = data.groupby(split_attribute_name) \n",
    "    data_subgroup = data_split.agg({target_attribute_name : [calculate_entropy_of_the_list, lambda x: len(x)/(len(data.index) * 1.0)] })[target_attribute_name]\n",
    "    data_subgroup.columns = ['Entropy', 'ProbObservations']\n",
    "    new_entropy = sum( data_subgroup['Entropy'] * data_subgroup['ProbObservations'] )\n",
    "    old_entropy = calculate_entropy_of_the_list(data[target_attribute_name])\n",
    "    return old_entropy-new_entropy\n",
    "\n",
    "def tree_with_IG3_algorithm(data, target_attribute_name, attribute_names, default_class=None):\n",
    "    from collections import Counter\n",
    "    count_target_attributes = Counter(x for x in data[target_attribute_name])\n",
    "    global node_position\n",
    "    if len(count_target_attributes) == 1:\n",
    "        return list(count_target_attributes.keys())[0]\n",
    "\n",
    "    elif data.empty or (not attribute_names):\n",
    "        return default_class \n",
    "    \n",
    "    else:\n",
    "        index_of_max = list(count_target_attributes.values()).index(max(count_target_attributes.values())) \n",
    "        default_class = list(count_target_attributes.keys())[index_of_max]  \n",
    "        \n",
    "        info_gain = [information_gain(data, attr, target_attribute_name) for attr in attribute_names]\n",
    "        index_of_max = info_gain.index(max(info_gain)) \n",
    "        best_attr = attribute_names[index_of_max]\n",
    "        tree = {best_attr:{}}\n",
    "        positiveCount = data['Class'].value_counts()[1];\n",
    "        negativeCount = data['Class'].value_counts()[0];\n",
    "        if positiveCount>negativeCount :\n",
    "            best_class = 1\n",
    "        elif positiveCount<negativeCount:\n",
    "            best_class = 0\n",
    "        else:\n",
    "            best_class = 'none'\n",
    "        tree[best_attr]['best_class'] = best_class\n",
    "        node_position = node_position + 1\n",
    "        tree[best_attr]['number'] = node_position\n",
    "        remaining_attribute_names = [i for i in attribute_names if i != best_attr]\n",
    "\n",
    "        for attr_val, data_subset in data.groupby(best_attr):\n",
    "            \n",
    "            subtree = tree_with_IG3_algorithm(data_subset,\n",
    "                        target_attribute_name,\n",
    "                        remaining_attribute_names,\n",
    "                        default_class)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree\n",
    "    \n",
    "\n",
    "#now I save the dictionaries of the trees built with IG3 and Variance Impurityree = tree_with_IG3_algorithm(training_set, 'Class', labelValues)\n",
    "tree_gain = tree_with_IG3_algorithm(training_set, 'Class', labelValues)\n",
    "tree_variance = tree_with_variance_impurity_algorithm(training_set, 'Class', labelValues)\n",
    "\n",
    "#The following function computes the accuracy of the tree\n",
    "def tree_accuracy(instance, tree, default=None):\n",
    "    attribute = list(tree.keys())[0]\n",
    "    if instance[attribute] in tree[attribute].keys():\n",
    "        result = tree[attribute][instance[attribute]]\n",
    "        if isinstance(result, dict): \n",
    "            return tree_accuracy(instance, result)\n",
    "        else:\n",
    "            return result \n",
    "    else:\n",
    "        return default\n",
    "\n",
    "#Now I start with the pruning algorithm\n",
    "\n",
    "#This first function orders the nodes in the new_tree D′ from 1 to N;\n",
    "def order_of_the_nodes (temp_tree, number):\n",
    "    if isinstance(temp_tree, dict):\n",
    "        attribute = list(temp_tree.keys())[0]\n",
    "        if temp_tree[attribute]['number'] == number:\n",
    "            if(temp_tree[attribute][0]!=0 and temp_tree[attribute][0]!=1):\n",
    "                new_tree = temp_tree[attribute][0]\n",
    "                if isinstance(new_tree, dict):\n",
    "                    temp_attribute = list(new_tree.keys())[0]\n",
    "                    temp_tree[attribute][0] = new_tree[temp_attribute]['best_class']\n",
    "            elif(temp_tree[attribute][1]!=0 and temp_tree[attribute][1]!=1):\n",
    "                new_tree = temp_tree[attribute][1]\n",
    "                if isinstance(new_tree, dict):\n",
    "                    temp_attribute = list(new_tree.keys())[0]      \n",
    "                    temp_tree[attribute][1] = new_tree[temp_attribute]['best_class']\n",
    "        else:\n",
    "            left = temp_tree[attribute][0]\n",
    "            right = temp_tree[attribute][1]\n",
    "            order_of_the_nodes(left, number)\n",
    "            order_of_the_nodes(right,number )\n",
    "    return temp_tree\n",
    "\n",
    "\n",
    "\n",
    "def number_of_internal_nodes(tree):\n",
    "    if isinstance(tree, dict):\n",
    "        attribute = list(tree.keys())[0]\n",
    "        left = tree[attribute][0]\n",
    "        right = tree[attribute][1]\n",
    "        return (1 + number_of_internal_nodes(left) +  \n",
    "               number_of_internal_nodes(right)); \n",
    "    else:\n",
    "        return 0;\n",
    "    \n",
    "\n",
    "def post_pruning(L, K, tree):\n",
    "    best_tree = tree\n",
    "    for i in range(1, L+1) :\n",
    "        new_tree = copy.deepcopy(best_tree)\n",
    "        M = randint(1, K);\n",
    "        for j in range(1, M+1):\n",
    "            n = number_of_internal_nodes(new_tree)\n",
    "            if n> 0:\n",
    "                P = randint(1,n)\n",
    "            else:\n",
    "                P = 0\n",
    "            order_of_the_nodes(new_tree, P)\n",
    "        test_set['accuracyBeforePruning'] = test_set.apply(tree_accuracy, axis=1, args=(best_tree,'1') ) \n",
    "        accuracyBeforePruning = str( sum(test_set['Class']==test_set['accuracyBeforePruning'] ) / (1.0*len(test_set.index)) )\n",
    "        test_set['accuracy_after_pruning'] = test_set.apply(tree_accuracy, axis=1, args=(new_tree,'1') ) \n",
    "        accuracy_after_pruning = str( sum(test_set['Class']==test_set['accuracy_after_pruning'] ) / (1.0*len(test_set.index)) )\n",
    "        if accuracy_after_pruning >= accuracyBeforePruning:\n",
    "            best_tree = new_tree\n",
    "    return best_tree\n",
    "\n",
    "\n",
    "#if to_print == 'yes':\n",
    "#    print(tree)\n",
    "#    print(tree2)\n",
    "   \n",
    "test_set['predicted_tree_gain'] = test_set.apply(tree_accuracy, axis=1, args=(tree_gain,'1') ) \n",
    "print( 'Accuracy with IG3 ' +  (str( sum(test_set['Class']==test_set['predicted_tree_gain'] ) / (0.01*len(test_set.index)) )))\n",
    "\n",
    "\n",
    "test_set['predicted_variance_impuruty'] = test_set.apply(tree_accuracy, axis=1, args=(tree_variance,'1') ) \n",
    "print( 'Accuracy with Variance Impuruty ' + (str( sum(test_set['Class']==test_set['predicted_variance_impuruty'] ) / (0.01*len(test_set.index)) )))\n",
    "\n",
    "\n",
    "pruned_tree_gain = post_pruning(L,K,tree_gain)\n",
    "pruned_tree_variance = post_pruning(L,K,tree_variance)\n",
    "\n",
    "test_set['predicted_pruned_tree_gain'] = test_set.apply(tree_accuracy, axis=1, args=(pruned_tree_gain,'1') ) \n",
    "print( 'Accuracy with pruned Information gain tree ' + (str( sum(test_set['Class']==test_set['predicted_pruned_tree_gain'] ) / (0.01*len(test_set.index)) )))\n",
    "test_set['predicted_pruned_tree_variance'] = test_set.apply(tree_accuracy, axis=1, args=(pruned_tree_variance,'1') ) \n",
    "print( 'Accuracy with pruned Variance gain tree ' + (str( sum(test_set['Class']==test_set['predicted_pruned_tree_variance'] ) / (0.01*len(test_set.index)) )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
